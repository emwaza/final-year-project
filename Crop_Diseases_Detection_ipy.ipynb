{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crop_Diseases_Detection.ipy",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwaza/final-year-project/blob/main/Crop_Diseases_Detection_ipy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_FLx9gt3Rb7"
      },
      "source": [
        "##**Crop Disease Detection Using Convolutional Neural Netework**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Convolutional Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm459EZSk4zS"
      },
      "source": [
        "### **Importing  the Librairies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ8YJMa4lfLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97bb0bf-c159-4a4d-a499-30ddf684d1ee"
      },
      "source": [
        "!pip install tensorflow==2.6.0\n",
        "!pip install keras==2.6.0\n",
        "!pip install h5py pyyaml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.6.0\n",
            "  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 11 kB/s \n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.43.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.12.0)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.37.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.2.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.1.2)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (2.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.6.0) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.1.1)\n",
            "Building wheels for collected packages: clang, wrapt\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=c3398ce0a131ca2aaed08c12c7b5ce225ea9f2a6af4e576cd8f964bc0c883ee4\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68724 sha256=d291c21408eb563ba04070fb47f923efeb6d82e91ce9567e860e871fc6900a86\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built clang wrapt\n",
            "Installing collected packages: typing-extensions, wrapt, flatbuffers, clang, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed clang-5.0 flatbuffers-1.12 tensorflow-2.6.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n",
            "Collecting keras==2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 28.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "Successfully installed keras-2.6.0\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9XR2lRUPtQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f1bfae-5f5b-4f2a-d6d3-b4d7568d4884"
      },
      "source": [
        "# Install nightly package for some functionalities that aren't in alpha\n",
        "!pip install tensorflow-gpu==1.15  # GPU\n",
        "\n",
        "\n",
        "# Install TF Hub for TF2\n",
        "!pip install 'tensorflow-hub == 0.7.0'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.6 kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.43.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=dc1c5ec391207b68e25cfd167447c79e00fe6e8253a22f7f91ebac162f8bc373\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires gast==0.4.0, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow 2.6.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires tensorflow-estimator~=2.6, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting tensorflow-hub==0.7.0\n",
            "  Downloading tensorflow_hub-0.7.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub==0.7.0) (1.19.5)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.12.0\n",
            "    Uninstalling tensorflow-hub-0.12.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.12.0\n",
            "Successfully installed tensorflow-hub-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_hZVquWBx9b"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "#from keras import optimizers\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoKSWDje_6z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39d5573-43d3-42ac-83cc-3f14f67c9b90"
      },
      "source": [
        "# verify TensorFlow version\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  1.15.0\n",
            "Eager mode:  False\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9qTqaSDaL-i"
      },
      "source": [
        "### Load the data\n",
        "Loading a dataset called PlantVillage from  https://drive.google.com/uc?id=18DbC6Xj4NP-hLzI14WuMaAEyq482vNfn. The images are around 54,305, representing different classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG5Hzp83Ezb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e347d01-8364-4aab-8744-44d2caaa1fd6"
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "file_id = '18DbC6Xj4NP-hLzI14WuMaAEyq482vNfn'\n",
        "\n",
        "# Download dataset\n",
        "!gdown https://drive.google.com/uc?id={file_id}\n",
        "# Unzip the downloaded file\n",
        "!unzip -q PlantVillage.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18DbC6Xj4NP-hLzI14WuMaAEyq482vNfn\n",
            "To: /content/PlantVillage.zip\n",
            "100% 866M/866M [00:07<00:00, 112MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA69CPvROvk5"
      },
      "source": [
        "### Prepare training and validation  dataset\n",
        "Create the training and validation directories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhB3buj3OoKP"
      },
      "source": [
        "# Dimension of resized image\n",
        "DEFAULT_IMAGE_SIZE = tuple((256, 256))\n",
        "\n",
        "# Number of images used to train the model\n",
        "N_IMAGES = 100\n",
        "\n",
        "# Path to the dataset folder\n",
        "root_dir = './PlantVillage'\n",
        "\n",
        "train_dir = os.path.join(root_dir, 'train')\n",
        "val_dir = os.path.join(root_dir, 'val')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDUxGnMdkmE8"
      },
      "source": [
        "import time\n",
        "import os\n",
        "from os.path import exists\n",
        "\n",
        "def count(dir, counter=0):\n",
        "    \"returns number of files in dir and subdirs\"\n",
        "    for pack in os.walk(dir):\n",
        "        for f in pack[2]:\n",
        "            counter += 1\n",
        "    return dir + \" : \" + str(counter) + \"files\"\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwFeDj4ughkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8367d732-e057-454c-a123-de0f8c163e5a"
      },
      "source": [
        "print('total images for training :', count(train_dir))\n",
        "print('total images for validation :', count(val_dir))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total images for training : ./PlantVillage/train : 44016files\n",
            "total images for validation : ./PlantVillage/val : 11004files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKbwKt0_aL_D"
      },
      "source": [
        "### Label mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-G_4n8ZF91f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb8f8cb-c20c-41ef-8ecd-7a7d667dd66c"
      },
      "source": [
        "!!wget https://github.com/obeshor/Plant-Diseases-Detector/archive/master.zip\n",
        "!unzip master.zip;"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  master.zip\n",
            "33dc8985c943175a7a1301fe034c01d69bebb8cf\n",
            "   creating: Plant-Diseases-Detector-master/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.gitignore  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/.idea/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/.idea/codeStyles/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/codeStyles/Project.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/codeStyles/codeStyleConfig.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/encodings.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/jarRepositories.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/misc.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/runConfigurations.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/.idea/vcs.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/build.gradle  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/proguard-rules.pro  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/androidTest/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/androidTest/java/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/androidTest/java/isomora/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/androidTest/java/isomora/com/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/androidTest/java/isomora/com/greendoctor/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/androidTest/java/isomora/com/greendoctor/ExampleInstrumentedTest.kt  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/AndroidManifest.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/assets/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/assets/plant_disease_model.tflite  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/assets/plant_labels.txt  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/assets/soybean.JPG  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/ic_launcher-web.png  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/java/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/java/isomora/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/java/isomora/com/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/java/isomora/com/greendoctor/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/java/isomora/com/greendoctor/Classifier.kt  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/java/isomora/com/greendoctor/MainActivity.kt  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/drawable-v24/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/drawable-v24/ic_launcher_foreground.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/drawable/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/drawable/ic_launcher_background.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/layout/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/layout/activity_main.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-anydpi-v26/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-anydpi-v26/ic_launcher.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-anydpi-v26/ic_launcher_round.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-hdpi/\n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-hdpi/ic_launcher.png  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-hdpi/ic_launcher_foreground.png  \n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-hdpi/ic_launcher_round.png  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-mdpi/\n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-mdpi/ic_launcher.png  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-mdpi/ic_launcher_foreground.png  \n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-mdpi/ic_launcher_round.png  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xhdpi/\n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xhdpi/ic_launcher.png  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xhdpi/ic_launcher_foreground.png  \n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xhdpi/ic_launcher_round.png  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxhdpi/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxhdpi/ic_launcher.png  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxhdpi/ic_launcher_foreground.png  \n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.png  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxxhdpi/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxxhdpi/ic_launcher.png  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxxhdpi/ic_launcher_foreground.png  \n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.png  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/values/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/values/colors.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/values/ic_launcher_background.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/values/strings.xml  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/main/res/values/styles.xml  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/test/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/test/java/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/test/java/isomora/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/test/java/isomora/com/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/app/src/test/java/isomora/com/greendoctor/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/app/src/test/java/isomora/com/greendoctor/ExampleUnitTest.kt  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/build.gradle  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/gradle.properties  \n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/gradle/\n",
            "   creating: Plant-Diseases-Detector-master/GreenDoctor/gradle/wrapper/\n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/gradle/wrapper/gradle-wrapper.jar  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/gradle/wrapper/gradle-wrapper.properties  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/gradlew  \n",
            "  inflating: Plant-Diseases-Detector-master/GreenDoctor/gradlew.bat  \n",
            " extracting: Plant-Diseases-Detector-master/GreenDoctor/settings.gradle  \n",
            "  inflating: Plant-Diseases-Detector-master/Plant_Diseases_Detection_with_TF2_V2.ipynb  \n",
            "  inflating: Plant-Diseases-Detector-master/Plant_Diseases_Detection_with_TF2_V4.ipynb  \n",
            "  inflating: Plant-Diseases-Detector-master/README.md  \n",
            "  inflating: Plant-Diseases-Detector-master/_config.yml  \n",
            "   creating: Plant-Diseases-Detector-master/assets/\n",
            "  inflating: Plant-Diseases-Detector-master/assets/PlantVillagefarmer.jpg  \n",
            "  inflating: Plant-Diseases-Detector-master/assets/detect_crop_disease_in_africa.jpg  \n",
            "  inflating: Plant-Diseases-Detector-master/assets/greendoctor.png  \n",
            "  inflating: Plant-Diseases-Detector-master/assets/over.png  \n",
            "  inflating: Plant-Diseases-Detector-master/categories.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCf_N7v3HBzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a001f7eb-3b97-46d0-9125-850a21414dc5"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('Plant-Diseases-Detector-master/categories.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)\n",
        "    classes = list(cat_to_name.values())\n",
        "    \n",
        "print (classes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzILXef8Um5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bb8ee2-f97e-4c7f-bac0-aadd05ac8171"
      },
      "source": [
        "print('Number of classes:',len(classes))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VeSPT_J0otV"
      },
      "source": [
        "###Select the Hub/TF2 module to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "lyCAYjNT09ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2641983-11cf-4e75-94c1-9c8ceb82e416"
      },
      "source": [
        "module_selection = (\"inception_v3\", 299, 2048) \n",
        "handle_base, pixels, FV_SIZE = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/2\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and output dimension {}\".format(\n",
        "  MODULE_HANDLE, IMAGE_SIZE, FV_SIZE))\n",
        "\n",
        "BATCH_SIZE = 64 "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2 with input size (299, 299) and output dimension 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypePvgaXw5Lm"
      },
      "source": [
        "### Data Preprocessing\n",
        "In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "JRrsFKez6fFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d767a124-dee3-45a5-d388-917ce46bd098"
      },
      "source": [
        "# Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning.\n",
        "\n",
        "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir, \n",
        "    shuffle=False, \n",
        "    seed=42,\n",
        "    color_mode=\"rgb\", \n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "do_data_augmentation = True #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2,                \n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2, \n",
        "      zoom_range=0.2,\n",
        "      fill_mode='nearest' )\n",
        "else:\n",
        "  train_datagen = validation_datagen\n",
        "  \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, \n",
        "    subset=\"training\", \n",
        "    shuffle=True, \n",
        "    seed=42,\n",
        "    color_mode=\"rgb\", \n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11004 images belonging to 39 classes.\n",
            "Found 44016 images belonging to 39 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Afm5Jn42E4T"
      },
      "source": [
        "###Build the model\n",
        "\n",
        "\n",
        "For speed, we start out with a non-trainable feature_extractor_layer, but WE can also enable fine-tuning for greater accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iiu7viPNdEI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5a80b0-51b0-4c8e-b7ea-9085a8bd80e7"
      },
      "source": [
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE+(3,),\n",
        "                                   output_shape=[FV_SIZE])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "hMYytuAGB34w"
      },
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}\n",
        "if do_fine_tuning:\n",
        "  feature_extractor.trainable = True\n",
        "  # unfreeze some layers of base network for fine-tuning\n",
        "  for layer in base_model.layers[-30:]:\n",
        "    layer.trainable =True\n",
        "  \n",
        "else:\n",
        "  feature_extractor.trainable = False\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9iG69R72XUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afe06f9-3c0c-4266-cf40-78ff03f16042"
      },
      "source": [
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
        "                           kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "#model.build((None,)+IMAGE_SIZE+(3,))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model with https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 2048)              21802784  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 39)                20007     \n",
            "=================================================================\n",
            "Total params: 22,871,879\n",
            "Trainable params: 1,069,095\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGGphhh8Vu50"
      },
      "source": [
        "### Specify Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "cq5rzR-vV7tn"
      },
      "source": [
        "#Compile model specifying the optimizer learning rate\n",
        "\n",
        "LEARNING_RATE = 0.001 #@param {type:\"number\"}\n",
        "\n",
        "model.compile(\n",
        "   optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "   loss='categorical_crossentropy',\n",
        "   metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia50ckJ6-rVr"
      },
      "source": [
        "### Train Model\n",
        "train model using validation dataset for validate each steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Cf1FVwyCRI8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5611aca1-26a7-4a67-aee3-feac11ccb998"
      },
      "source": [
        "\n",
        "EPOCHS=10 #@param {type:\"integer\"}\n",
        "\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=validation_generator.samples//validation_generator.batch_size)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "687/688 [============================>.] - ETA: 1s - loss: 0.7634 - acc: 0.7805Epoch 1/10\n",
            "688/688 [==============================] - 907s 1s/step - loss: 0.7631 - acc: 0.7806 - val_loss: 0.4501 - val_acc: 0.8589\n",
            "Epoch 2/10\n",
            " 13/688 [..............................] - ETA: 3:18 - loss: 0.4527 - acc: 0.8642"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLa4bHwbPNWD"
      },
      "source": [
        "###Check Performance\n",
        "Plot training and validation accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUPxHwHC3Gy_"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "#acc = history.history['accuracy']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "Qu1zcxO7177i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"maize_disease_detection.h5\")"
      ],
      "metadata": {
        "id": "tFeOe6g1ilb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNzdageWS6k5"
      },
      "source": [
        "### Random test\n",
        "Random sample images from validation dataset and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1jIf7rKTBLx"
      },
      "source": [
        "# Import OpenCV\n",
        "import cv2\n",
        "\n",
        "# Utility\n",
        "import itertools\n",
        "import random\n",
        "from collections import Counter\n",
        "from glob import iglob\n",
        "\n",
        "\n",
        "def load_image(filename):\n",
        "\n",
        "    img = cv2.imread(os.path.join(root_dir, val_dir, filename))\n",
        "    img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]) )\n",
        "    img = img /255\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "def predict(image):\n",
        "    probabilities = model.predict(np.asarray([img]))[0]\n",
        "    class_idx = np.argmax(probabilities)\n",
        "    \n",
        "    return {classes[class_idx]: probabilities[class_idx]}\n",
        "    image = random.sample(validation_generator.filenames, 16)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKZduGZoTFSg"
      },
      "source": [
        "for idx, filename in enumerate(random.sample(validation_generator.filenames, 1)):\n",
        "    print(\"SOURCE: class: %s, file: %s\" % (os.path.split(filename)[0], filename))\n",
        "    #img = load_image(filename)\n",
        "    img = load_image('/content/PlantVillage/train/Apple___Apple_scab/01f3deaa-6143-4b6c-9c22-620a46d8be04___FREC_Scab 3112.JPG')\n",
        "    prediction = predict(img)\n",
        "    print(\"PREDICTED: class: %s, confidence: %f\" % (list(prediction.keys())[0], list(prediction.values())[0]))\n",
        "    plt.imshow(img)\n",
        "    plt.figure(idx)    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4RfKIpKziKbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIm7-LQb9e1u"
      },
      "source": [
        "# Converting the model to tensorflowLite\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf "
      ],
      "metadata": {
        "id": "WYMgrOaYGQdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t = time.time()\n",
        "\n",
        "export_path = \"/tmp/saved_models/{}\".format(int(t))\n",
        "tf.keras.experimental.export_saved_model(model, export_path)\n",
        "\n",
        "export_path\n"
      ],
      "metadata": {
        "id": "mw88Ao9NIbft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now confirm that we can reload it, and it still gives the same results\n",
        "reloaded = tf.keras.experimental.load_from_saved_model(export_path, custom_objects={'KerasLayer':hub.KerasLayer})\n",
        "def predict_reload("
      ],
      "metadata": {
        "id": "KfoZ9vULINzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the model to TFLite\n",
        "!mkdir \"tflite_models\"\n",
        "TFLITE_MODEL = \"tflite_models/plant_disease_model.tflite\"\n",
        "\n",
        "\n",
        "# Get the concrete function from the Keras model.\n",
        "run_model = tf.function(lambda x : reloaded(x))\n",
        "\n",
        "# Save the concrete function.\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n",
        ")\n",
        "\n",
        "# Convert the model to standard TensorFlow Lite model\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converted_tflite_model = converter.convert()\n",
        "open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)"
      ],
      "metadata": {
        "id": "mKyJUuBMIO3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}